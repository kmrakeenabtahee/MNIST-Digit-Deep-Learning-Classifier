{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 \n",
    "\n",
    "This notebook contains the code code to train and evaluate a deep learning classifier on the MNIST dataset. It does this with the help of the keras framework, build_deep_nn from a1.py and with the use of keras_tuner to define the optimal parameters of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "tf.config.experimental.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.10.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing\n",
    "\n",
    "First we must import the MNIST dataset from `keras.datasets`, then we load the data into the instance. As the training and testing data are already on the MNIST dataset, we only have to separate the training and testing with the appropriate labels from the set of four Numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "train_images = train_images.astype('float32') / 255 # normalize the training images\n",
    "\n",
    "test_images = test_images.astype('float32') / 255 # normalize the testing images\n",
    "\n",
    "train_images.shape\n",
    "test_images.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `train_images` and `train_labels` are for the training set whereas the `test_images` and `test_labels` are for the testing set\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building \n",
    "\n",
    "Once the training and testing data has been loaded, we can now construct a neural network using the following parameters from the MNIST training data to train the neural network.\n",
    "\n",
    "\n",
    "From above, we can see that the shape, i.e. the input size is 28 * 28 and so input rows and columns are `28`. When building the model below, we need to consider the fact that MNIST images are grayscale and so the number of channels would be `1`. For the `hidden_sizes`, I have used a minimum number of neurons of `32` to a maximum of `512 `where stepping would be done at `32`.\n",
    "\n",
    "For each model, we will try a different number of hidden layers from 1 to 3. Each layer will have the same number of neurons. This can be seen from the code line ``hp.Int('num_hidden', min_value=1, max_value=3)``\n",
    "\n",
    "For the dropout layer, we are using a min_value of `0` and max of `0.5` with a step of 0.1.\n",
    "\n",
    "I have also used the ``sparse_categorical_crossentropy`` loss function so that we can compile the model without having to separately one-hot encode the data.\n",
    "\n",
    "Using ``adam`` I could ensure that we got the best of both worlds from Momentum and RMSProp. Moreover, ``adam`` can change its learning rate adaptively. This would enable us to come to a good solution faster by accurately and efficiently navigating through the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_tuner import RandomSearch\n",
    "import a1\n",
    "\n",
    "## This code was adapted from three sources: https://www.tensorflow.org/tutorials/keras/keras_tuner, https://www.analyticsvidhya.com/blog/2021/08/hyperparameter-tuning-of-neural-networks-using-keras-tuner/ and https://chat.openai.com/\n",
    "\n",
    "def build_model(hp):\n",
    "    # Initialize the Sequential API and start stacking the layers using build_deep_nn from a1.py\n",
    "    model = a1.build_deep_nn(\n",
    "        28,\n",
    "        28,\n",
    "        1,\n",
    "        hp.Int('num_hidden', min_value=1, max_value=3),\n",
    "        (hp.Int('hidden_size', min_value=32, max_value=512, step=32),) * 3,\n",
    "        (hp.Float('dropout_rate', min_value=0.0, max_value=0.5, step=0.1),) * 3,\n",
    "        10,\n",
    "        'softmax'\n",
    "    )\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy', \n",
    "                metrics=['accuracy'])\n",
    "    \n",
    "    print(model.fit(train_images, train_labels, epochs=5, validation_split=0.1))\n",
    "    \n",
    "    evaluation = model.evaluate(test_images, test_labels, return_dict=True)\n",
    "    print(\"\\nEvaluation Metrics:\")\n",
    "    for metric, value in evaluation.items():\n",
    "        print(f\"{metric}: {value}\")\n",
    "\n",
    "    print(\"\\nModel Summary:\")\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created the model, we can use it to tune the parameters to answer our questions:\n",
    "1. What are the hyperparameters of the optimal model?\n",
    "2. What are the accuracy results of the optimal model on the test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "Using the RandomSearch from keras.tuner, I will now perform hyperparameter tuning on the model above. It is used to search the space for the optimal configuration for our neural network model by randomly sampling hyperparameter combinations. \n",
    "\n",
    "The `tuner.search` method performs the following steps for each trial:\n",
    "1.  Builds the model using the specified hyperparameters.\n",
    "2.  Trains the model on the training data for the specified number of epochs.\n",
    "3.  Evaluates the model's performance on the validation set.\n",
    "4.  Updates the tuner's internal state based on the trial's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 00m 37s]\n",
      "accuracy: 0.9322592616081238\n",
      "\n",
      "Best accuracy So Far: 0.9944815039634705\n",
      "Total elapsed time: 00h 03m 23s\n"
     ]
    }
   ],
   "source": [
    "# This tuner was adapted from https://www.tensorflow.org/tutorials/keras/keras_tuner\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    # Function to build hte model\n",
    "    build_model, \n",
    "    \n",
    "    # Objective to optimize the model for\n",
    "    objective='accuracy',\n",
    "    \n",
    "    #Maximum sets of different hyperparameters to try\n",
    "    max_trials=5,\n",
    "    \n",
    "    seed = 42\n",
    ")\n",
    "\n",
    "tuner.search(train_images, train_labels, epochs=5, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation\n",
    "\n",
    "In this section, I will train the model using the optimal hyperparameters that I have found from the Keras Tuner and evaluate the performance. The metrics are displayed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1688/1688 [==============================] - 6s 3ms/step - loss: 0.2244 - accuracy: 0.9334 - val_loss: 0.1001 - val_accuracy: 0.9712\n",
      "Epoch 2/5\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.0947 - accuracy: 0.9715 - val_loss: 0.0797 - val_accuracy: 0.9770\n",
      "Epoch 3/5\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.0640 - accuracy: 0.9804 - val_loss: 0.0698 - val_accuracy: 0.9818\n",
      "Epoch 4/5\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.0478 - accuracy: 0.9847 - val_loss: 0.0730 - val_accuracy: 0.9800\n",
      "Epoch 5/5\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.0338 - accuracy: 0.9896 - val_loss: 0.0660 - val_accuracy: 0.9817\n",
      "<keras.callbacks.History object at 0x00000224E0B77160>\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.0683 - accuracy: 0.9794\n",
      "\n",
      "Evaluation Metrics:\n",
      "loss: 0.0683172270655632\n",
      "accuracy: 0.9793999791145325\n",
      "\n",
      "Model Summary:\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_1 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 480)               376800    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 480)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                4810      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 381,610\n",
      "Trainable params: 381,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.0294 - accuracy: 0.9903 - val_loss: 0.0692 - val_accuracy: 0.9803\n",
      "Epoch 2/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.0249 - accuracy: 0.9921 - val_loss: 0.0772 - val_accuracy: 0.9822\n",
      "Epoch 3/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.0193 - accuracy: 0.9936 - val_loss: 0.0800 - val_accuracy: 0.9820\n",
      "Epoch 4/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.0189 - accuracy: 0.9934 - val_loss: 0.0690 - val_accuracy: 0.9838\n",
      "Epoch 5/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.0165 - accuracy: 0.9942 - val_loss: 0.0753 - val_accuracy: 0.9845\n",
      "Epoch 6/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.0152 - accuracy: 0.9950 - val_loss: 0.0855 - val_accuracy: 0.9822\n",
      "Epoch 7/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.0117 - accuracy: 0.9960 - val_loss: 0.0838 - val_accuracy: 0.9838\n",
      "Epoch 8/10\n",
      "1688/1688 [==============================] - 6s 3ms/step - loss: 0.0137 - accuracy: 0.9951 - val_loss: 0.0876 - val_accuracy: 0.9830\n",
      "Epoch 9/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.0109 - accuracy: 0.9960 - val_loss: 0.0930 - val_accuracy: 0.9835\n",
      "Epoch 10/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.0107 - accuracy: 0.9962 - val_loss: 0.0933 - val_accuracy: 0.9847\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.0890 - accuracy: 0.9817\n"
     ]
    }
   ],
   "source": [
    "# Get the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# Build the model with the best hyperparameters and train it\n",
    "model = build_model(best_hps)\n",
    "model.fit(train_images, train_labels, epochs=10, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "\n",
    "num_hidden = best_hps.get('num_hidden')\n",
    "hidden_size = best_hps.get('hidden_size')\n",
    "dropout_rate = best_hps.get('dropout_rate')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9817000031471252\n",
      "\n",
      "Best hyperparameters: \n",
      "num_hidden:  1\n",
      "hidden_size:  480\n",
      "dropout_rate:  0.1\n"
     ]
    }
   ],
   "source": [
    "print(\"Test accuracy:\", test_acc)\n",
    "print(\"\\nBest hyperparameters: \")\n",
    "\n",
    "print(\"num_hidden: \", num_hidden) \n",
    "print(\"hidden_size: \", hidden_size) \n",
    "print(\"dropout_rate: \", dropout_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Above we can see the `best hyperparameters` and below we can see the `best accuracy` for those hyperparameters under Evaluation metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1688/1688 [==============================] - 6s 3ms/step - loss: 0.2196 - accuracy: 0.9358 - val_loss: 0.1063 - val_accuracy: 0.9673\n",
      "Epoch 2/5\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.0940 - accuracy: 0.9721 - val_loss: 0.0855 - val_accuracy: 0.9743\n",
      "Epoch 3/5\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.0637 - accuracy: 0.9801 - val_loss: 0.0682 - val_accuracy: 0.9815\n",
      "Epoch 4/5\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.0473 - accuracy: 0.9846 - val_loss: 0.0769 - val_accuracy: 0.9783\n",
      "Epoch 5/5\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.0345 - accuracy: 0.9896 - val_loss: 0.0694 - val_accuracy: 0.9807\n",
      "<keras.callbacks.History object at 0x00000224E9E18730>\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.0665 - accuracy: 0.9818\n",
      "\n",
      "Evaluation Metrics:\n",
      "loss: 0.06653013825416565\n",
      "accuracy: 0.9818000197410583\n",
      "\n",
      "Model Summary:\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_2 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 480)               376800    \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 480)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 10)                4810      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 381,610\n",
      "Trainable params: 381,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "best_model = build_model(best_hps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results and Discussion\n",
    "\n",
    "### Hyperparameters of the optimal model are:\n",
    "\n",
    "- Number of hidden layers: 1\n",
    "\n",
    "- Size of the hidden layers: 480\n",
    "\n",
    "- Dropout rate of the final hidden layer: 0.1\n",
    "\n",
    "### Accuracy results:\n",
    "\n",
    "- Evaluated accuracy: 0.9818 $\\approx$ 98.2% "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use of AI generators in this assignment\n",
    "\n",
    "I acknowledge the use of ChatGPT in the drafting and proofreading of this assignment.\n",
    "\n",
    "I have used ChatGPT in various parts of this assignment to ask about keras_tuner and neural_network algorithms and codes to be inspired by to learn and create this assignment. I have furthered my understanding by visiting the sources including the websites: https://www.analyticsvidhya.com/blog/, https://www.tensorflow.org/tutorials/keras/keras_tuner, and https://stackoverflow.com/ to further my understanding of the best practice on top of the lecture and workshop content. I have used these websites as source of inspiration and learning purely, including proofreading on ChatGPT.\n",
    "\n",
    "General prompts such as: `How do I build a deep neural network` for the deep neural network on a1.py and autogenerated lines of text to finish the sentences of the notebook and code analysis such as `how could I improve my model` for code analysis to understand the flaws with complete explanations for my understanding of the code itself and reasoning. I have also used ChatGPT to navigate through the variables and parameter names of the libraries."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
